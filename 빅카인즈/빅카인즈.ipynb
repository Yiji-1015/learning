{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4da2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (4.49.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.54.0-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: hf-xet, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.29.2\n",
      "    Uninstalling huggingface-hub-0.29.2:\n",
      "      Successfully uninstalled huggingface-hub-0.29.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.49.0\n",
      "    Uninstalling transformers-4.49.0:\n",
      "      Successfully uninstalled transformers-4.49.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-upstage 0.6.0 requires tokenizers<0.20.0,>=0.19.1, but you have tokenizers 0.21.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed hf-xet-1.1.5 huggingface-hub-0.34.1 tokenizers-0.21.2 transformers-4.54.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c07069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.1.0\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from torch==2.1.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from torch==2.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from torch==2.1.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from torch==2.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from torch==2.1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from torch==2.1.0) (2025.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.1.0 (from torch==2.1.0)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
      "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
      "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
      "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
      "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\n",
      "torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 triton-2.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers==4.40.1 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from transformers==4.40.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers==4.40.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers==4.40.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers==4.40.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages (from requests->transformers==4.40.1) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.1.0\n",
    "!pip install transformers==4.40.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a570483e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\ncannot import name 'get_cached_models' from 'transformers.utils' (/home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1863\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:36\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     cached_file,\n\u001b[1;32m     30\u001b[0m     extract_commit_hash,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     logging,\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoder_decoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderDecoderConfig\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyAutoMapping\n",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/models/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     albert,\n\u001b[1;32m     17\u001b[0m     align,\n\u001b[1;32m     18\u001b[0m     altclip,\n\u001b[1;32m     19\u001b[0m     audio_spectrogram_transformer,\n\u001b[1;32m     20\u001b[0m     auto,\n\u001b[1;32m     21\u001b[0m     autoformer,\n\u001b[1;32m     22\u001b[0m     bark,\n\u001b[1;32m     23\u001b[0m     bart,\n\u001b[1;32m     24\u001b[0m     barthez,\n\u001b[1;32m     25\u001b[0m     bartpho,\n\u001b[1;32m     26\u001b[0m     beit,\n\u001b[1;32m     27\u001b[0m     bert,\n\u001b[1;32m     28\u001b[0m     bert_generation,\n\u001b[1;32m     29\u001b[0m     bert_japanese,\n\u001b[1;32m     30\u001b[0m     bertweet,\n\u001b[1;32m     31\u001b[0m     big_bird,\n\u001b[1;32m     32\u001b[0m     bigbird_pegasus,\n\u001b[1;32m     33\u001b[0m     biogpt,\n\u001b[1;32m     34\u001b[0m     bit,\n\u001b[1;32m     35\u001b[0m     blenderbot,\n\u001b[1;32m     36\u001b[0m     blenderbot_small,\n\u001b[1;32m     37\u001b[0m     blip,\n\u001b[1;32m     38\u001b[0m     blip_2,\n\u001b[1;32m     39\u001b[0m     bloom,\n\u001b[1;32m     40\u001b[0m     bridgetower,\n\u001b[1;32m     41\u001b[0m     bros,\n\u001b[1;32m     42\u001b[0m     byt5,\n\u001b[1;32m     43\u001b[0m     camembert,\n\u001b[1;32m     44\u001b[0m     canine,\n\u001b[1;32m     45\u001b[0m     chinese_clip,\n\u001b[1;32m     46\u001b[0m     clap,\n\u001b[1;32m     47\u001b[0m     clip,\n\u001b[1;32m     48\u001b[0m     clipseg,\n\u001b[1;32m     49\u001b[0m     clvp,\n\u001b[1;32m     50\u001b[0m     code_llama,\n\u001b[1;32m     51\u001b[0m     codegen,\n\u001b[1;32m     52\u001b[0m     cohere,\n\u001b[1;32m     53\u001b[0m     conditional_detr,\n\u001b[1;32m     54\u001b[0m     convbert,\n\u001b[1;32m     55\u001b[0m     convnext,\n\u001b[1;32m     56\u001b[0m     convnextv2,\n\u001b[1;32m     57\u001b[0m     cpm,\n\u001b[1;32m     58\u001b[0m     cpmant,\n\u001b[1;32m     59\u001b[0m     ctrl,\n\u001b[1;32m     60\u001b[0m     cvt,\n\u001b[1;32m     61\u001b[0m     data2vec,\n\u001b[1;32m     62\u001b[0m     dbrx,\n\u001b[1;32m     63\u001b[0m     deberta,\n\u001b[1;32m     64\u001b[0m     deberta_v2,\n\u001b[1;32m     65\u001b[0m     decision_transformer,\n\u001b[1;32m     66\u001b[0m     deformable_detr,\n\u001b[1;32m     67\u001b[0m     deit,\n\u001b[1;32m     68\u001b[0m     deprecated,\n\u001b[1;32m     69\u001b[0m     depth_anything,\n\u001b[1;32m     70\u001b[0m     deta,\n\u001b[1;32m     71\u001b[0m     detr,\n\u001b[1;32m     72\u001b[0m     dialogpt,\n\u001b[1;32m     73\u001b[0m     dinat,\n\u001b[1;32m     74\u001b[0m     dinov2,\n\u001b[1;32m     75\u001b[0m     distilbert,\n\u001b[1;32m     76\u001b[0m     dit,\n\u001b[1;32m     77\u001b[0m     donut,\n\u001b[1;32m     78\u001b[0m     dpr,\n\u001b[1;32m     79\u001b[0m     dpt,\n\u001b[1;32m     80\u001b[0m     efficientformer,\n\u001b[1;32m     81\u001b[0m     efficientnet,\n\u001b[1;32m     82\u001b[0m     electra,\n\u001b[1;32m     83\u001b[0m     encodec,\n\u001b[1;32m     84\u001b[0m     encoder_decoder,\n\u001b[1;32m     85\u001b[0m     ernie,\n\u001b[1;32m     86\u001b[0m     ernie_m,\n\u001b[1;32m     87\u001b[0m     esm,\n\u001b[1;32m     88\u001b[0m     falcon,\n\u001b[1;32m     89\u001b[0m     fastspeech2_conformer,\n\u001b[1;32m     90\u001b[0m     flaubert,\n\u001b[1;32m     91\u001b[0m     flava,\n\u001b[1;32m     92\u001b[0m     fnet,\n\u001b[1;32m     93\u001b[0m     focalnet,\n\u001b[1;32m     94\u001b[0m     fsmt,\n\u001b[1;32m     95\u001b[0m     funnel,\n\u001b[1;32m     96\u001b[0m     fuyu,\n\u001b[1;32m     97\u001b[0m     gemma,\n\u001b[1;32m     98\u001b[0m     git,\n\u001b[1;32m     99\u001b[0m     glpn,\n\u001b[1;32m    100\u001b[0m     gpt2,\n\u001b[1;32m    101\u001b[0m     gpt_bigcode,\n\u001b[1;32m    102\u001b[0m     gpt_neo,\n\u001b[1;32m    103\u001b[0m     gpt_neox,\n\u001b[1;32m    104\u001b[0m     gpt_neox_japanese,\n\u001b[1;32m    105\u001b[0m     gpt_sw3,\n\u001b[1;32m    106\u001b[0m     gptj,\n\u001b[1;32m    107\u001b[0m     gptsan_japanese,\n\u001b[1;32m    108\u001b[0m     graphormer,\n\u001b[1;32m    109\u001b[0m     grounding_dino,\n\u001b[1;32m    110\u001b[0m     groupvit,\n\u001b[1;32m    111\u001b[0m     herbert,\n\u001b[1;32m    112\u001b[0m     hubert,\n\u001b[1;32m    113\u001b[0m     ibert,\n\u001b[1;32m    114\u001b[0m     idefics,\n\u001b[1;32m    115\u001b[0m     idefics2,\n\u001b[1;32m    116\u001b[0m     imagegpt,\n\u001b[1;32m    117\u001b[0m     informer,\n\u001b[1;32m    118\u001b[0m     instructblip,\n\u001b[1;32m    119\u001b[0m     jamba,\n\u001b[1;32m    120\u001b[0m     jukebox,\n\u001b[1;32m    121\u001b[0m     kosmos2,\n\u001b[1;32m    122\u001b[0m     layoutlm,\n\u001b[1;32m    123\u001b[0m     layoutlmv2,\n\u001b[1;32m    124\u001b[0m     layoutlmv3,\n\u001b[1;32m    125\u001b[0m     layoutxlm,\n\u001b[1;32m    126\u001b[0m     led,\n\u001b[1;32m    127\u001b[0m     levit,\n\u001b[1;32m    128\u001b[0m     lilt,\n\u001b[1;32m    129\u001b[0m     llama,\n\u001b[1;32m    130\u001b[0m     llava,\n\u001b[1;32m    131\u001b[0m     llava_next,\n\u001b[1;32m    132\u001b[0m     longformer,\n\u001b[1;32m    133\u001b[0m     longt5,\n\u001b[1;32m    134\u001b[0m     luke,\n\u001b[1;32m    135\u001b[0m     lxmert,\n\u001b[1;32m    136\u001b[0m     m2m_100,\n\u001b[1;32m    137\u001b[0m     mamba,\n\u001b[1;32m    138\u001b[0m     marian,\n\u001b[1;32m    139\u001b[0m     markuplm,\n\u001b[1;32m    140\u001b[0m     mask2former,\n\u001b[1;32m    141\u001b[0m     maskformer,\n\u001b[1;32m    142\u001b[0m     mbart,\n\u001b[1;32m    143\u001b[0m     mbart50,\n\u001b[1;32m    144\u001b[0m     mega,\n\u001b[1;32m    145\u001b[0m     megatron_bert,\n\u001b[1;32m    146\u001b[0m     megatron_gpt2,\n\u001b[1;32m    147\u001b[0m     mgp_str,\n\u001b[1;32m    148\u001b[0m     mistral,\n\u001b[1;32m    149\u001b[0m     mixtral,\n\u001b[1;32m    150\u001b[0m     mluke,\n\u001b[1;32m    151\u001b[0m     mobilebert,\n\u001b[1;32m    152\u001b[0m     mobilenet_v1,\n\u001b[1;32m    153\u001b[0m     mobilenet_v2,\n\u001b[1;32m    154\u001b[0m     mobilevit,\n\u001b[1;32m    155\u001b[0m     mobilevitv2,\n\u001b[1;32m    156\u001b[0m     mpnet,\n\u001b[1;32m    157\u001b[0m     mpt,\n\u001b[1;32m    158\u001b[0m     mra,\n\u001b[1;32m    159\u001b[0m     mt5,\n\u001b[1;32m    160\u001b[0m     musicgen,\n\u001b[1;32m    161\u001b[0m     musicgen_melody,\n\u001b[1;32m    162\u001b[0m     mvp,\n\u001b[1;32m    163\u001b[0m     nat,\n\u001b[1;32m    164\u001b[0m     nezha,\n\u001b[1;32m    165\u001b[0m     nllb,\n\u001b[1;32m    166\u001b[0m     nllb_moe,\n\u001b[1;32m    167\u001b[0m     nougat,\n\u001b[1;32m    168\u001b[0m     nystromformer,\n\u001b[1;32m    169\u001b[0m     olmo,\n\u001b[1;32m    170\u001b[0m     oneformer,\n\u001b[1;32m    171\u001b[0m     openai,\n\u001b[1;32m    172\u001b[0m     opt,\n\u001b[1;32m    173\u001b[0m     owlv2,\n\u001b[1;32m    174\u001b[0m     owlvit,\n\u001b[1;32m    175\u001b[0m     patchtsmixer,\n\u001b[1;32m    176\u001b[0m     patchtst,\n\u001b[1;32m    177\u001b[0m     pegasus,\n\u001b[1;32m    178\u001b[0m     pegasus_x,\n\u001b[1;32m    179\u001b[0m     perceiver,\n\u001b[1;32m    180\u001b[0m     persimmon,\n\u001b[1;32m    181\u001b[0m     phi,\n\u001b[1;32m    182\u001b[0m     phobert,\n\u001b[1;32m    183\u001b[0m     pix2struct,\n\u001b[1;32m    184\u001b[0m     plbart,\n\u001b[1;32m    185\u001b[0m     poolformer,\n\u001b[1;32m    186\u001b[0m     pop2piano,\n\u001b[1;32m    187\u001b[0m     prophetnet,\n\u001b[1;32m    188\u001b[0m     pvt,\n\u001b[1;32m    189\u001b[0m     pvt_v2,\n\u001b[1;32m    190\u001b[0m     qdqbert,\n\u001b[1;32m    191\u001b[0m     qwen2,\n\u001b[1;32m    192\u001b[0m     qwen2_moe,\n\u001b[1;32m    193\u001b[0m     rag,\n\u001b[1;32m    194\u001b[0m     realm,\n\u001b[1;32m    195\u001b[0m     recurrent_gemma,\n\u001b[1;32m    196\u001b[0m     reformer,\n\u001b[1;32m    197\u001b[0m     regnet,\n\u001b[1;32m    198\u001b[0m     rembert,\n\u001b[1;32m    199\u001b[0m     resnet,\n\u001b[1;32m    200\u001b[0m     roberta,\n\u001b[1;32m    201\u001b[0m     roberta_prelayernorm,\n\u001b[1;32m    202\u001b[0m     roc_bert,\n\u001b[1;32m    203\u001b[0m     roformer,\n\u001b[1;32m    204\u001b[0m     rwkv,\n\u001b[1;32m    205\u001b[0m     sam,\n\u001b[1;32m    206\u001b[0m     seamless_m4t,\n\u001b[1;32m    207\u001b[0m     seamless_m4t_v2,\n\u001b[1;32m    208\u001b[0m     segformer,\n\u001b[1;32m    209\u001b[0m     seggpt,\n\u001b[1;32m    210\u001b[0m     sew,\n\u001b[1;32m    211\u001b[0m     sew_d,\n\u001b[1;32m    212\u001b[0m     siglip,\n\u001b[1;32m    213\u001b[0m     speech_encoder_decoder,\n\u001b[1;32m    214\u001b[0m     speech_to_text,\n\u001b[1;32m    215\u001b[0m     speech_to_text_2,\n\u001b[1;32m    216\u001b[0m     speecht5,\n\u001b[1;32m    217\u001b[0m     splinter,\n\u001b[1;32m    218\u001b[0m     squeezebert,\n\u001b[1;32m    219\u001b[0m     stablelm,\n\u001b[1;32m    220\u001b[0m     starcoder2,\n\u001b[1;32m    221\u001b[0m     superpoint,\n\u001b[1;32m    222\u001b[0m     swiftformer,\n\u001b[1;32m    223\u001b[0m     swin,\n\u001b[1;32m    224\u001b[0m     swin2sr,\n\u001b[1;32m    225\u001b[0m     swinv2,\n\u001b[1;32m    226\u001b[0m     switch_transformers,\n\u001b[1;32m    227\u001b[0m     t5,\n\u001b[1;32m    228\u001b[0m     table_transformer,\n\u001b[1;32m    229\u001b[0m     tapas,\n\u001b[1;32m    230\u001b[0m     time_series_transformer,\n\u001b[1;32m    231\u001b[0m     timesformer,\n\u001b[1;32m    232\u001b[0m     timm_backbone,\n\u001b[1;32m    233\u001b[0m     trocr,\n\u001b[1;32m    234\u001b[0m     tvlt,\n\u001b[1;32m    235\u001b[0m     tvp,\n\u001b[1;32m    236\u001b[0m     udop,\n\u001b[1;32m    237\u001b[0m     umt5,\n\u001b[1;32m    238\u001b[0m     unispeech,\n\u001b[1;32m    239\u001b[0m     unispeech_sat,\n\u001b[1;32m    240\u001b[0m     univnet,\n\u001b[1;32m    241\u001b[0m     upernet,\n\u001b[1;32m    242\u001b[0m     videomae,\n\u001b[1;32m    243\u001b[0m     vilt,\n\u001b[1;32m    244\u001b[0m     vipllava,\n\u001b[1;32m    245\u001b[0m     vision_encoder_decoder,\n\u001b[1;32m    246\u001b[0m     vision_text_dual_encoder,\n\u001b[1;32m    247\u001b[0m     visual_bert,\n\u001b[1;32m    248\u001b[0m     vit,\n\u001b[1;32m    249\u001b[0m     vit_hybrid,\n\u001b[1;32m    250\u001b[0m     vit_mae,\n\u001b[1;32m    251\u001b[0m     vit_msn,\n\u001b[1;32m    252\u001b[0m     vitdet,\n\u001b[1;32m    253\u001b[0m     vitmatte,\n\u001b[1;32m    254\u001b[0m     vits,\n\u001b[1;32m    255\u001b[0m     vivit,\n\u001b[1;32m    256\u001b[0m     wav2vec2,\n\u001b[1;32m    257\u001b[0m     wav2vec2_bert,\n\u001b[1;32m    258\u001b[0m     wav2vec2_conformer,\n\u001b[1;32m    259\u001b[0m     wav2vec2_phoneme,\n\u001b[1;32m    260\u001b[0m     wav2vec2_with_lm,\n\u001b[1;32m    261\u001b[0m     wavlm,\n\u001b[1;32m    262\u001b[0m     whisper,\n\u001b[1;32m    263\u001b[0m     x_clip,\n\u001b[1;32m    264\u001b[0m     xglm,\n\u001b[1;32m    265\u001b[0m     xlm,\n\u001b[1;32m    266\u001b[0m     xlm_prophetnet,\n\u001b[1;32m    267\u001b[0m     xlm_roberta,\n\u001b[1;32m    268\u001b[0m     xlm_roberta_xl,\n\u001b[1;32m    269\u001b[0m     xlnet,\n\u001b[1;32m    270\u001b[0m     xmod,\n\u001b[1;32m    271\u001b[0m     yolos,\n\u001b[1;32m    272\u001b[0m     yoso,\n\u001b[1;32m    273\u001b[0m )\n",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/models/depth_anything/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyModule, is_torch_available\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptionalDependencyNotAvailable\n",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/file_utils.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Backward compatibility imports, to make sure all those objects can be found in file_utils\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     28\u001b[0m     CONFIG_NAME,\n\u001b[1;32m     29\u001b[0m     DUMMY_INPUTS,\n\u001b[1;32m     30\u001b[0m     DUMMY_MASK,\n\u001b[1;32m     31\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m     32\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m     33\u001b[0m     FEATURE_EXTRACTOR_NAME,\n\u001b[1;32m     34\u001b[0m     FLAX_WEIGHTS_NAME,\n\u001b[1;32m     35\u001b[0m     HF_MODULES_CACHE,\n\u001b[1;32m     36\u001b[0m     HUGGINGFACE_CO_PREFIX,\n\u001b[1;32m     37\u001b[0m     HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n\u001b[1;32m     38\u001b[0m     MODEL_CARD_NAME,\n\u001b[1;32m     39\u001b[0m     MULTIPLE_CHOICE_DUMMY_INPUTS,\n\u001b[1;32m     40\u001b[0m     PYTORCH_PRETRAINED_BERT_CACHE,\n\u001b[1;32m     41\u001b[0m     PYTORCH_TRANSFORMERS_CACHE,\n\u001b[1;32m     42\u001b[0m     S3_BUCKET_PREFIX,\n\u001b[1;32m     43\u001b[0m     SENTENCEPIECE_UNDERLINE,\n\u001b[1;32m     44\u001b[0m     SPIECE_UNDERLINE,\n\u001b[1;32m     45\u001b[0m     TF2_WEIGHTS_NAME,\n\u001b[1;32m     46\u001b[0m     TF_WEIGHTS_NAME,\n\u001b[1;32m     47\u001b[0m     TORCH_FX_REQUIRED_VERSION,\n\u001b[1;32m     48\u001b[0m     TRANSFORMERS_CACHE,\n\u001b[1;32m     49\u001b[0m     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n\u001b[1;32m     50\u001b[0m     USE_JAX,\n\u001b[1;32m     51\u001b[0m     USE_TF,\n\u001b[1;32m     52\u001b[0m     USE_TORCH,\n\u001b[1;32m     53\u001b[0m     WEIGHTS_INDEX_NAME,\n\u001b[1;32m     54\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     55\u001b[0m     ContextManagers,\n\u001b[1;32m     56\u001b[0m     DummyObject,\n\u001b[1;32m     57\u001b[0m     EntryNotFoundError,\n\u001b[1;32m     58\u001b[0m     ExplicitEnum,\n\u001b[1;32m     59\u001b[0m     ModelOutput,\n\u001b[1;32m     60\u001b[0m     PaddingStrategy,\n\u001b[1;32m     61\u001b[0m     PushToHubMixin,\n\u001b[1;32m     62\u001b[0m     RepositoryNotFoundError,\n\u001b[1;32m     63\u001b[0m     RevisionNotFoundError,\n\u001b[1;32m     64\u001b[0m     TensorType,\n\u001b[1;32m     65\u001b[0m     _LazyModule,\n\u001b[1;32m     66\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     67\u001b[0m     add_end_docstrings,\n\u001b[1;32m     68\u001b[0m     add_start_docstrings,\n\u001b[1;32m     69\u001b[0m     add_start_docstrings_to_model_forward,\n\u001b[1;32m     70\u001b[0m     cached_property,\n\u001b[1;32m     71\u001b[0m     copy_func,\n\u001b[1;32m     72\u001b[0m     default_cache_path,\n\u001b[1;32m     73\u001b[0m     define_sagemaker_information,\n\u001b[1;32m     74\u001b[0m     get_cached_models,\n\u001b[1;32m     75\u001b[0m     get_file_from_repo,\n\u001b[1;32m     76\u001b[0m     get_torch_version,\n\u001b[1;32m     77\u001b[0m     has_file,\n\u001b[1;32m     78\u001b[0m     http_user_agent,\n\u001b[1;32m     79\u001b[0m     is_apex_available,\n\u001b[1;32m     80\u001b[0m     is_bs4_available,\n\u001b[1;32m     81\u001b[0m     is_coloredlogs_available,\n\u001b[1;32m     82\u001b[0m     is_datasets_available,\n\u001b[1;32m     83\u001b[0m     is_detectron2_available,\n\u001b[1;32m     84\u001b[0m     is_faiss_available,\n\u001b[1;32m     85\u001b[0m     is_flax_available,\n\u001b[1;32m     86\u001b[0m     is_ftfy_available,\n\u001b[1;32m     87\u001b[0m     is_g2p_en_available,\n\u001b[1;32m     88\u001b[0m     is_in_notebook,\n\u001b[1;32m     89\u001b[0m     is_ipex_available,\n\u001b[1;32m     90\u001b[0m     is_librosa_available,\n\u001b[1;32m     91\u001b[0m     is_offline_mode,\n\u001b[1;32m     92\u001b[0m     is_onnx_available,\n\u001b[1;32m     93\u001b[0m     is_pandas_available,\n\u001b[1;32m     94\u001b[0m     is_phonemizer_available,\n\u001b[1;32m     95\u001b[0m     is_protobuf_available,\n\u001b[1;32m     96\u001b[0m     is_psutil_available,\n\u001b[1;32m     97\u001b[0m     is_py3nvml_available,\n\u001b[1;32m     98\u001b[0m     is_pyctcdecode_available,\n\u001b[1;32m     99\u001b[0m     is_pytesseract_available,\n\u001b[1;32m    100\u001b[0m     is_pytorch_quantization_available,\n\u001b[1;32m    101\u001b[0m     is_rjieba_available,\n\u001b[1;32m    102\u001b[0m     is_sagemaker_dp_enabled,\n\u001b[1;32m    103\u001b[0m     is_sagemaker_mp_enabled,\n\u001b[1;32m    104\u001b[0m     is_scipy_available,\n\u001b[1;32m    105\u001b[0m     is_sentencepiece_available,\n\u001b[1;32m    106\u001b[0m     is_seqio_available,\n\u001b[1;32m    107\u001b[0m     is_sklearn_available,\n\u001b[1;32m    108\u001b[0m     is_soundfile_availble,\n\u001b[1;32m    109\u001b[0m     is_spacy_available,\n\u001b[1;32m    110\u001b[0m     is_speech_available,\n\u001b[1;32m    111\u001b[0m     is_tensor,\n\u001b[1;32m    112\u001b[0m     is_tensorflow_probability_available,\n\u001b[1;32m    113\u001b[0m     is_tf2onnx_available,\n\u001b[1;32m    114\u001b[0m     is_tf_available,\n\u001b[1;32m    115\u001b[0m     is_timm_available,\n\u001b[1;32m    116\u001b[0m     is_tokenizers_available,\n\u001b[1;32m    117\u001b[0m     is_torch_available,\n\u001b[1;32m    118\u001b[0m     is_torch_bf16_available,\n\u001b[1;32m    119\u001b[0m     is_torch_cuda_available,\n\u001b[1;32m    120\u001b[0m     is_torch_fx_available,\n\u001b[1;32m    121\u001b[0m     is_torch_fx_proxy,\n\u001b[1;32m    122\u001b[0m     is_torch_mps_available,\n\u001b[1;32m    123\u001b[0m     is_torch_tf32_available,\n\u001b[1;32m    124\u001b[0m     is_torch_xla_available,\n\u001b[1;32m    125\u001b[0m     is_torchaudio_available,\n\u001b[1;32m    126\u001b[0m     is_training_run_on_sagemaker,\n\u001b[1;32m    127\u001b[0m     is_vision_available,\n\u001b[1;32m    128\u001b[0m     replace_return_docstrings,\n\u001b[1;32m    129\u001b[0m     requires_backends,\n\u001b[1;32m    130\u001b[0m     to_numpy,\n\u001b[1;32m    131\u001b[0m     to_py_obj,\n\u001b[1;32m    132\u001b[0m     torch_only_method,\n\u001b[1;32m    133\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_cached_models' from 'transformers.utils' (/home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/utils/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1852\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1851\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1865\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\ncannot import name 'get_cached_models' from 'transformers.utils' (/home/khuaibm/old_backup/root_workspace/yiji/env/lib/python3.10/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c7a834",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsyche/KoT5-summarization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsyche/KoT5-summarization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"psyche/KoT5-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"psyche/KoT5-summarization\")\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "sample = \"\"\"\n",
    "역대 최저 출산율 속에서도 국내 키즈용품 시장이 오히려 성장하고 있다. 출산율은 줄었지만, 한 아이에게 지갑을 여는 가족 구성원은 많아졌기 때문이다. 부모는 물론 조부모, 삼촌, 이모 등 최대 10명에 이르는 친인척이 한 아이에게 집중적으로 소비하며 ‘VIB(Very Important Baby)’ 트렌드가 확산되고 있다.\n",
    "통계청이 발표한 2025년 4월 인구동향에 따르면, 해당 월 출생아 수는 2만717명으로 전년 동월보다 1658명(8.7%) 증가했다. 이는 2022년 4월 이후 처음으로 2만 명대를 회복한 수치이며, 4월 기준 출생률 증가율로는 1991년 이후 34년 만에 최고치다. 이러한 추세가 이어질 경우, 올해 합계출산율이 0.8명을 넘어설 가능성도 제기되고 있다.\n",
    "# ‘골드키즈’ 겨냥한 키즈 브랜드 고성장\n",
    "라이프스타일 플랫폼 29CM(이십구센티미터)는 최근 약 3개월간(4월 1일~7월 17일) ‘29선물하기’ 서비스 데이터를 분석한 결과, 키즈 카테고리 거래액이 전년 동기간 대비 5배 이상 증가한 것으로 나타났다. 주요 고객층인 2539세대 여성들이 조카나 지인의 자녀를 위한 선물 수요를 주도하며, 감각적인 디자인의 키즈 브랜드에 대한 관심이 높아지고 있다.\n",
    "실제로 여름 시즌을 맞아 수영복과 모자는 인기 선물 아이템으로 부상했다. 영유아 의류 브랜드 ‘콘크리트브레드’의 소스레시피 수영복과 스윔보넷은 원색 컬러와 귀여운 그래픽 디자인으로 주목받으며 키즈 선물 카테고리 1위를 차지했다. 영유아 패브릭 브랜드 ‘모노레임’의 바디슈트와 애착 인형 역시 출산 선물로 큰 인기를 끌고 있다. 특히 바디슈트는 오가닉 순면 소재와 민들레 일러스트 패턴, 선물용 패키지 구성이 돋보인다. 애착 인형은 편안함을 느끼도록 납작한 수건 형태로 제작돼 실용성을 더했다.\n",
    "29CM는 고객의 취향을 반영해 ‘베베노피노’, ‘세아랑’, ‘콘크리트브레드’, ‘모노레임’ 등 독창적인 디자인을 갖춘 키즈 브랜드 큐레이션을 강화하고 있으며, 다양한 가족의 이야기를 담은 오리지널 콘텐츠 ‘29 I, Like’를 통해 브랜드 접점을 확대하고 있다.\n",
    "29CM는 최근 약 3개월간(4월 1일~7월 17일) ‘29선물하기’ 서비스 데이터를 분석한 결과, 키즈 카테고리 거래액이 전년 동기간 대비 5배 이상 증가한 것으로 나타났다.\n",
    "신세계백화점 역시 키즈 시장 공략에 나섰다. 올해 2월부터 마크곤잘레스 키즈, 마리떼 키즈, LEE 키즈, 커버낫 키즈 등 성인 스트리트 감성을 반영한 키즈 브랜드를 대거 도입해 정식 입점까지 이끌었다. 기존의 귀엽고 아기자기한 이미지에서 벗어나 성인 패션 트렌드를 키즈에 접목하며 아동복의 눈높이를 한층 높였다.\n",
    "그 결과, 신세계백화점의 아동복 부문은 2월부터 6월까지 전년 동기 대비 27% 이상 매출 상승을 기록했다. 트렌디한 브랜드 라인업이 패션에 민감한 2030 부모층의 소비욕을 자극한 것이 주요 배경으로 분석된다.\n",
    "‘골드키즈’를 위한 소비 트렌드는 건강기능식품 시장으로도 확장되고 있다. 한국건강기능식품협회에 따르면, 2023년 국내 키즈 건강기능식품 시장은 약 2599억 원 규모로, 2020년 대비 52% 성장했다. 최근에는 식약처로부터 어린이 키 성장 기능성을 인정받은 유산균발효굴추출물(FGO) 성분이 각광받고 있다.\n",
    "정관장은 이를 바탕으로 출시한 ‘아이키커 하이’ 제품으로 히트를 기록했다. 이 제품은 출시 첫날 5억 원 이상의 매출을 올리며 주목받았으며, 주요 구매층은 40대 학부모로 집계됐다. 자녀의 성장 골든타임에 맞춰 24주분 구성이 전체 판매량의 74%를 차지하며 여름방학 수요와 맞물려 큰 인기를 끌고 있다.\n",
    "골드키즈 트렌드는 국내뿐만 아니라 해외에서도 나타나고 있다.\n",
    "국내 유아 브랜드 궁중비책은 지난 7월 16일부터 18일까지 중국 상하이에서 열린 ‘제25회 CBME 2025’ 박람회에 참가해 현지 바이어들의 큰 호응을 얻었다. CBME는 세계 3대 유아용품 박람회로, 4,500여 개 브랜드와 약 10만 명의 업계 관계자가 참가하는 글로벌 행사다.\n",
    "궁중비책은 이 자리에서 국내 아동용 자외선 차단제 최초로 중국 국가약품감독관리국(NMPA) 등록을 마친 선케어 라인을 포함해, 키즈·베이비 라인을 선보였다. 특히 인기 캐릭터 ‘가필드’와의 협업, 한국적인 전통 미감을 살린 부스 디자인, 현지 KOL 초청 라이브 방송 등 전략적 마케팅이 주목받았다.\n",
    "전시 기간 동안 궁중비책 부스는 약 3000명의 관람객이 방문했고, 현재는 티몰, 징동, 월마트, 샘스클럽 등 온·오프라인 유통 채널을 통해 중국 내 입지를 넓히고 있다.\n",
    "# “하나뿐인 아이에게 프리미엄을”… 키즈 시장 고급화 전략\n",
    "출산율 하락에도 불구하고 키즈 시장은 ‘질적 성장’으로 활로를 찾고 있다. ‘한 명뿐인 아이에게는 아낌없이 투자한다’는 골드키즈 소비 트렌드는 디자인과 기능성, 정체성을 겸비한 키즈 브랜드의 성장 원동력이 되고 있다.\n",
    "유통가와 브랜드들은 이러한 흐름을 반영해 프리미엄화, 큐레이션, 글로벌 진출 등으로 키즈 시장의 새로운 가능성을 열고 있다.\n",
    "업계 관계자는 “과거에는 키즈 브랜드가 기능성과 가격 경쟁력에 초점을 맞췄다면, 지금은 디자인, 브랜드 정체성, 큐레이션 서비스가 핵심 경쟁력으로 떠올랐다”며 “특히 Z세대 부모와 이모·삼촌 소비자층의 유입은 키즈 시장을 트렌디하고 감성적인 영역으로 이끌고 있으며, 이는 향후 세분화된 프리미엄 시장 확대로 이어질 것”이라고 말했다.\"\"\"\n",
    "\n",
    "inputs = [prefix + sample]\n",
    "\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=3, do_sample=True, min_length=100, max_length=500)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "# result = nltk.sent_tokenize(decoded_output.strip())\n",
    "\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    " \n",
    "chat = ChatUpstage(model=\"solar-pro2-250710\")\n",
    "\n",
    "\n",
    "def get_messages(x):\n",
    "    return [\n",
    "        SystemMessage(content=\"사용자의 질문에 대해 왕족 말투로 답변해 주세요.\"),\n",
    "        HumanMessage(content=x)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "babe863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = {\n",
    "    \"ID\": 1,\n",
    "    \"name\": \"최성호\",\n",
    "    \"gender\": \"남\",\n",
    "    \"region\": \"경기도\",\n",
    "    \"age\": \"60~64세\",\n",
    "    \"married\": \"사별\",\n",
    "    \"income\": \"4분위\",\n",
    "    \"education\": \"대학교-졸업\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    패널 정보: {panel}\n",
    "    정치 뉴스 목록: {news_list_ml_json}\n",
    "\n",
    "    당신은 패널의 정치 성향과 판단 흐름을 분석하는 전문 psychologist입니다.\n",
    "    각 패널은 다음 정보를 포함합니다:\n",
    "    - vote_party: 2024년 4월 총선 당시 지지 정당\n",
    "\n",
    "    당신의 목표는 다음과 같습니다.\n",
    "\n",
    "    - 각 패널의 개인정보와 2024vote(총선 당시 지지 정당), 주요 뉴스 데이터를 바탕으로\n",
    "    정치 판단 과정의 흐름이 드러나는 인터뷰 문장을 생성하세요.\n",
    "\n",
    "    - 다음과 같은 **Chain-of-Thought(CoT)** 구조를 따라야 합니다:\n",
    "    ① 과거 판단 또는 가치 → ② 최근 뉴스나 사회 변화에 대한 반응 → ③ 현재 판단 또는 민감한 이슈\n",
    "\n",
    "    - 인터뷰는 최대 100자 이내이며, 개인정보 서술없이 정치적 가치, 뉴스 이슈, 현재 판단 등의 개인적 의견만 포함합니다.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c74de024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아, 그대의 궁금증이 참으로 기꺼워 보이는구나. **\"빅카인즈(BIG KINDS)\"**는 한국언론진흥재단에서 개발한 **빅데이터 기반 뉴스 분석 플랫폼**으로, 수많은 뉴스 데이터를 체계적으로 수집·분석하여 다양한 인사이트를 제공하는 **디지털 지식의 보고**라 할 수 있느니라.\n",
      "\n",
      "### 📜 주요 특징\n",
      "1. **방대한 데이터** : 50여 개 언론사의 뉴스, SNS, 커뮤니티 데이터를 실시간으로 수집하며, 과거 30년 분량의 아카이브도 보유하고 있도다.\n",
      "2. **심층 분석** : 키워드 트렌드, 감성 분석, 주제어 추출 등을 통해 사회 현상을 다각도로 해석할 수 있도록 돕는구나.\n",
      "3. **시각적 도구** : 워드 클라우드, 트렌드 그래프, 연관어 맵 등으로 복잡한 데이터를 직관적으로 이해할 수 있게 하니, 마치 **미디어의 별자리**를 보는 듯하다.\n",
      "\n",
      "### 👑 활용 예시\n",
      "- **정책 결정자** : 여론 반응을 분석해 정책 방향을 수립하거나\n",
      "- **기업인** : 소비자 감성 추이를 파악해 마케팅 전략을 조정하거나\n",
      "- **학자** : 사회적 이슈를 계량적으로 연구할 때 유용하게 쓰일 수 있느니라.\n",
      "\n",
      "### 🔍 접근 방법\n",
      "일반 사용자도 무료로 기본 기능을 이용할 수 있으나, 보다 심층적인 분석을 원한다면 **유료 구독**을 통해 고급 도구를 활용할 수 있도다. 한국언론진흥재단 공식 웹사이트에서 그대의 지적 탐구를 시작하시도록 하라.\n",
      "\n",
      "이 플랫폼은 **디지털 시대의 현대적 점술가**와도 같아, 뉴스라는 거대한 바다에서 의미를 캐내는 데 탁월한 능력을 지니고 있느니라. 그대에게도 유용한 도구가 되길 바라노라. 🌌\n"
     ]
    }
   ],
   "source": [
    "response = chat.invoke(get_messages(\"빅카인즈에 대해 알려줘\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85772f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
